# Machine Learning: 让机器找到一个复杂到很难直接编程得出 function

| network | ouput |
| ------- | ----- |
| Regression | scalar |
| Classification | class |
| Structured Learning/ Autoregressive Generation | infinite structured data (image, document, ...) |

<table>
  <tr>
    <td>训练损失很高的原因</td>
    <td>解释</td>
    <td>解决方案</td>
  </tr>
  <tr>
    <td rowspan="2">Model Bias</td>
    <td rowspan="2">model 本身的 limitation/ assumption 不足够表达/概括实际的情况</td>
    <td>更大更深的网络</td>
  </tr>
  <tr>
    <td>Testing time scaling (albert, openAI)</td>
  </tr>
  <tr>
    <td rowspan="2">Optimization</td>
    <td rowspan="2">表达力够了，但优化时没找到目标</td>
    <td>Small Batch</td>
  </tr>
  <tr>
    <td>Momentum</td>
  </tr>
</table>

| Optimization 方法 | 作用 | 解释 |
| - | - | - |
| 不同深度的网络对比训练损失 | 确认是 optimization 而不是 model bias | 更深的网络有更多的训练损失证明没有训练到位（因为更深的模型可以什么都不做来模拟更浅的模型） |
| Small Batch | 训练速度变慢，但是可以引入噪音，会使得 training 和 validate 同时变好 | 更 noisy 就不容易卡在 sharp minimal 和 saddle point: 可以用 Tayler Series Approximation 去描述当前位置附近的 Loss function 形状（如果eigenvalue 全部为正就是 local minimal）经验上绝大多数训练卡住都是在 saddle point |
| Momentum | 上一次移动方向 - 当前 gradient | 虽然每次只考虑当前和上一次的 gradient，但是上一次的 gradient 也受到上上一次的影响。所以可以理解成每一步都考虑了所有 gradient |

| 验证损失很高 | 原因 | 解决方案 |
| - | - | - | 
| overfitting | 在 training dataset 上有变好，但是在 unseen data 上变差了。模型能力太强，数据不够的地方开始 freestyle 了 | 需要平衡 model complexity 和 bias：1. 更多的 training data (data augmentation); 2. 限制模型：less/ sharing parameters 减少 feature (CNN 之于 FCN); 3. early stopping, regularization, dropout |
| mismatch | 训练分布不同于测试分布（数据没收集好/data drift）| 重新收集数据进行训练 |


