# Machine Learning: 让机器找到一个复杂到很难直接编程得到的 function

<table>
  <tr>
    <td>Network</td>
    <td>Ouput</td>
  </tr>
  <tr>
    <td>Regression</td>
    <td>Scalar</td>
  </tr>
  <tr>
    <td>Classification</td>
    <td>Class</td>
  </tr>
  <tr>
    <td>Autoregressive Generation</td>
    <td>Infinite, structured data</td>
  </tr>
<table>

<table>
  <tr>
    <td>训练损失很高的原因</td>
    <td>解决方案</td>
    <td>解释</td>
  </tr>
  <tr>
    <td rowspan="2"><b>Model Bias</b>: model 本身的 limitation/ assumption 不足够表达/概括实际的情况</td>
    <td>Deeper, wider</td>
    <td>增加模型弹性</td>
  </tr>
  <tr>
    <td>Testing time scaling</td>
    <td>albert 的论文证明了重复同一个模型更长时间也会有更好的结果</td>
  </tr>
  <tr>
    <td rowspan="5"><b>Optimization</b>: 表达力够了，但优化时没找到目标。不同深度的网络对比训练损失，更深的网络有更多的训练损失证明没有训练到位</td>
    <td rowspan="2">Small Batch</td>
    <td>训练速度变慢，但是可以引入噪音，就不容易卡在 sharp minimal 和 saddle point，generalize 会使得 training 和 validate 同时变好</td>
  </tr>
  <tr>
      <td>Tayler Series Approximation 可以描述当前位置附近的 Loss function 形状：如果eigenvalue 全部为正就是 local minimal，经验上绝大多数训练卡住都是在 saddle point</td>
  </tr>
  <tr>
    <td rowspan="2">Adam: RMSProp + Momentum</td>
    <td>Momentum: 上一次移动方向 - 当前 gradient。虽然每次只考虑当前和上一次的 gradient，但是上一次的 gradient 也受到上上一次的影响：每一步都考虑了所有 gradient。会考虑方向和大小</td>
  </tr>
  <tr>
    <td>Adaptive Learning Rate: RMSProp。lr 太高根本没到不了 critical point。不同的参数 + 同一个参数不同的时候都需要有不同的 lr。只考虑大小，不考虑方向</td>
  </tr>
  <tr>
    <td>Warm up</td>
    <td>Bert, Transformer, RNN：一开始先慢慢走，到处看一下收集一点统计数据 sigma</td>
  </tr>
</table>

<table>
  <tr>
    <td>验证损失很高的原因</td>
    <td>解决方案</td>
    <td>解释</td>
  </tr>
  <tr>
    <td rowspan="3">overfitting: 在 training dataset 上有变好，但是在 unseen data 上变差了。模型能力太强，数据不够的地方是 freestyle</td>
    <td>更多的 training data (data augmentation)</td>
    <td>数据越多模型 freestyle 空间越少</td>
  </tr>
  <tr>
    <td>限制模型：less/ sharing parameters 减少 feature</td>
    <td>比如用 CNN 代替 FCN</td>
  </tr>
  <tr>
    <td>early stopping, regularization, dropout</td>
    <td>不改变网络架构的前提</td>
  </tr>
  <tr>
    <td>mismatch: 数据没收集好/data drift</td>
    <td>重新收集数据进行训练</td>
    <td>训练分布不同于测试分布</td>
  </tr>
</table>